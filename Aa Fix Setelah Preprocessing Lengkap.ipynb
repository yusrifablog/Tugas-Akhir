{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data_train_lengkap.csv', sep=';')\n",
    "data_test = pd.read_csv('data_test_lengkap.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>price</th>\n",
       "      <th>packaging</th>\n",
       "      <th>product</th>\n",
       "      <th>aroma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dair penasaran banget skincare reviewnya banya...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ngincer bilang dupenya ski baca review tertawa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>suka tabirsurya tekstur ringan serap banget ku...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nyobain pas muka lagi stress ny desperate bang...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suka gokujyun losion merek hada labo lotionnya...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  price  packaging  \\\n",
       "0  dair penasaran banget skincare reviewnya banya...     -1          0   \n",
       "1  ngincer bilang dupenya ski baca review tertawa...      0          0   \n",
       "2  suka tabirsurya tekstur ringan serap banget ku...      0          1   \n",
       "3  nyobain pas muka lagi stress ny desperate bang...      0          0   \n",
       "4  suka gokujyun losion merek hada labo lotionnya...      1          0   \n",
       "\n",
       "   product  aroma  \n",
       "0      0.0      0  \n",
       "1      1.0      0  \n",
       "2      1.0      0  \n",
       "3     -1.0      0  \n",
       "4      1.0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>price</th>\n",
       "      <th>packaging</th>\n",
       "      <th>product</th>\n",
       "      <th>aroma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>produk favorit banget enak dipake plus tidak r...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lengket pas dipake hitungan tabirsurya yang ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minyak kontrol lumayan ngempesin jerawat efek ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>favorite banget harga terjangkau banget gak ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exfoliant toner pakai bagi step double toning ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  price  packaging  \\\n",
       "0  produk favorit banget enak dipake plus tidak r...     -1          0   \n",
       "1  lengket pas dipake hitungan tabirsurya yang ti...      1          0   \n",
       "2  minyak kontrol lumayan ngempesin jerawat efek ...      1          0   \n",
       "3  favorite banget harga terjangkau banget gak ha...      1          0   \n",
       "4  exfoliant toner pakai bagi step double toning ...      0          0   \n",
       "\n",
       "   product  aroma  \n",
       "0      1.0      0  \n",
       "1      0.0      0  \n",
       "2     -1.0      0  \n",
       "3      1.0      0  \n",
       "4      1.0      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_text    0\n",
       "price          0\n",
       "packaging      0\n",
       "product        0\n",
       "aroma          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_text    0\n",
       "price          0\n",
       "packaging      0\n",
       "product        0\n",
       "aroma          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train['review_text']\n",
    "X_test = data_test['review_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "fitur_train = TfidfVectorizer().fit(X_train)\n",
    "tfidf_train = fitur_train.transform(X_train).toarray()\n",
    "tfidf_test = fitur_train.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitur_tfidf(tfidf, fitur):\n",
    "    terms = tfidf.get_feature_names()\n",
    "    sums = fitur.sum(axis = 0)\n",
    "\n",
    "    data = []\n",
    "    for col, term in enumerate(terms):\n",
    "        data.append((term, sums[col] ))\n",
    "    ranking = pd.DataFrame(data, columns=['Terms','TF-IDF'])\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidf_tr = get_fitur_tfidf(fitur_train, tfidf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7361, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_tfidf_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE SELECTION CHI SQUARE\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_chi_tr = tfidf_train\n",
    "x_chi_test = tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_chi_tr = np.array(data_train[['price','packaging','aroma','product']])\n",
    "#y_chi_test = np.array(data_test[['price','packaging','aroma','product']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_price_tr = (data_train['price'])\n",
    "y_packaging_tr = (data_train['packaging'])\n",
    "y_product_tr = (data_train['product'])\n",
    "y_aroma_tr = (data_train['aroma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_price_test = (data_test['price'])\n",
    "y_packaging_test = (data_test['packaging'])\n",
    "y_product_test = (data_test['product'])\n",
    "y_aroma_test = (data_test['aroma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_price_tr = SelectKBest(score_func=chi2, k=76).fit_transform(x_chi_tr, y_price_tr)\n",
    "chi_packaging_tr = SelectKBest(score_func=chi2, k=76).fit_transform(x_chi_tr, y_packaging_tr)\n",
    "chi_product_tr = SelectKBest(score_func=chi2, k=76).fit_transform(x_chi_tr, y_product_tr)\n",
    "chi_aroma_tr = SelectKBest(score_func=chi2, k=76).fit_transform(x_chi_tr, y_aroma_tr)\n",
    "\n",
    "\n",
    "chi_price_test = SelectKBest(score_func=chi2, k=76).fit_transform(x_chi_test, y_price_test)\n",
    "chi_packaging_test = SelectKBest(score_func=chi2, k=76).fit_transform(x_chi_test, y_packaging_test)\n",
    "chi_product_test = SelectKBest(score_func=chi2, k=76).fit_transform(x_chi_test, y_product_test)\n",
    "chi_aroma_test = SelectKBest(score_func=chi2, k=76).fit_transform(x_chi_test, y_aroma_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#aspek price\n",
    "knn_price_2 = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_price_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_price_7 = KNeighborsClassifier(n_neighbors=7)\n",
    "knn_price_9 = KNeighborsClassifier(n_neighbors=9)\n",
    "knn_price_13 = KNeighborsClassifier(n_neighbors=13)\n",
    "knn_price_20 = KNeighborsClassifier(n_neighbors=20)\n",
    "knn_price_50 = KNeighborsClassifier(n_neighbors=50)\n",
    "\n",
    "knn_price_2.fit(chi_price_tr, y_price_tr)\n",
    "knn_price_5.fit(chi_price_tr, y_price_tr)\n",
    "knn_price_7.fit(chi_price_tr, y_price_tr)\n",
    "knn_price_9.fit(chi_price_tr, y_price_tr)\n",
    "knn_price_13.fit(chi_price_tr, y_price_tr)\n",
    "knn_price_20.fit(chi_price_tr, y_price_tr)\n",
    "knn_price_50.fit(chi_price_tr, y_price_tr)\n",
    "\n",
    "predictions_price_2 = knn_price_2.predict(chi_price_test)\n",
    "predictions_price_5 = knn_price_5.predict(chi_price_test)\n",
    "predictions_price_7 = knn_price_7.predict(chi_price_test)\n",
    "predictions_price_9 = knn_price_9.predict(chi_price_test)\n",
    "predictions_price_13 = knn_price_13.predict(chi_price_test)\n",
    "predictions_price_20 = knn_price_20.predict(chi_price_test)\n",
    "predictions_price_50 = knn_price_50.predict(chi_price_test)\n",
    "\n",
    "#aspek packaging\n",
    "knn_packaging_2 = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_packaging_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_packaging_7 = KNeighborsClassifier(n_neighbors=7)\n",
    "knn_packaging_9 = KNeighborsClassifier(n_neighbors=9)\n",
    "knn_packaging_13 = KNeighborsClassifier(n_neighbors=13)\n",
    "knn_packaging_20 = KNeighborsClassifier(n_neighbors=20)\n",
    "knn_packaging_50 = KNeighborsClassifier(n_neighbors=50)\n",
    "\n",
    "knn_packaging_2.fit(chi_packaging_tr, y_packaging_tr)\n",
    "knn_packaging_5.fit(chi_packaging_tr, y_packaging_tr)\n",
    "knn_packaging_7.fit(chi_packaging_tr, y_packaging_tr)\n",
    "knn_packaging_9.fit(chi_packaging_tr, y_packaging_tr)\n",
    "knn_packaging_13.fit(chi_packaging_tr, y_packaging_tr)\n",
    "knn_packaging_20.fit(chi_packaging_tr, y_packaging_tr)\n",
    "knn_packaging_50.fit(chi_packaging_tr, y_packaging_tr)\n",
    "\n",
    "predictions_packaging_2 = knn_packaging_2.predict(chi_packaging_test)\n",
    "predictions_packaging_5 = knn_packaging_5.predict(chi_packaging_test)\n",
    "predictions_packaging_7 = knn_packaging_7.predict(chi_packaging_test)\n",
    "predictions_packaging_9 = knn_packaging_9.predict(chi_packaging_test)\n",
    "predictions_packaging_13 = knn_packaging_13.predict(chi_packaging_test)\n",
    "predictions_packaging_20 = knn_packaging_20.predict(chi_packaging_test)\n",
    "predictions_packaging_50 = knn_packaging_50.predict(chi_packaging_test)\n",
    "\n",
    "#aspek product\n",
    "knn_product_2 = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_product_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_product_7 = KNeighborsClassifier(n_neighbors=7)\n",
    "knn_product_9 = KNeighborsClassifier(n_neighbors=9)\n",
    "knn_product_13 = KNeighborsClassifier(n_neighbors=13)\n",
    "knn_product_20 = KNeighborsClassifier(n_neighbors=20)\n",
    "knn_product_50 = KNeighborsClassifier(n_neighbors=50)\n",
    "\n",
    "knn_product_2.fit(chi_product_tr, y_product_tr)\n",
    "knn_product_5.fit(chi_product_tr, y_product_tr)\n",
    "knn_product_7.fit(chi_product_tr, y_product_tr)\n",
    "knn_product_9.fit(chi_product_tr, y_product_tr)\n",
    "knn_product_13.fit(chi_product_tr, y_product_tr)\n",
    "knn_product_20.fit(chi_product_tr, y_product_tr)\n",
    "knn_product_50.fit(chi_product_tr, y_product_tr)\n",
    "\n",
    "predictions_product_2 = knn_product_2.predict(chi_product_test)\n",
    "predictions_product_5 = knn_product_5.predict(chi_product_test)\n",
    "predictions_product_7 = knn_product_7.predict(chi_product_test)\n",
    "predictions_product_9 = knn_product_9.predict(chi_product_test)\n",
    "predictions_product_13 = knn_product_13.predict(chi_product_test)\n",
    "predictions_product_20 = knn_product_20.predict(chi_product_test)\n",
    "predictions_product_50 = knn_product_50.predict(chi_product_test)\n",
    "\n",
    "#aspek aroma\n",
    "knn_aroma_2 = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_aroma_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_aroma_7 = KNeighborsClassifier(n_neighbors=7)\n",
    "knn_aroma_9 = KNeighborsClassifier(n_neighbors=9)\n",
    "knn_aroma_13 = KNeighborsClassifier(n_neighbors=13)\n",
    "knn_aroma_20 = KNeighborsClassifier(n_neighbors=20)\n",
    "knn_aroma_50 = KNeighborsClassifier(n_neighbors=50)\n",
    "\n",
    "knn_aroma_2.fit(chi_aroma_tr, y_aroma_tr)\n",
    "knn_aroma_5.fit(chi_aroma_tr, y_aroma_tr)\n",
    "knn_aroma_7.fit(chi_aroma_tr, y_aroma_tr)\n",
    "knn_aroma_9.fit(chi_aroma_tr, y_aroma_tr)\n",
    "knn_aroma_13.fit(chi_aroma_tr, y_aroma_tr)\n",
    "knn_aroma_20.fit(chi_aroma_tr, y_aroma_tr)\n",
    "knn_aroma_50.fit(chi_aroma_tr, y_aroma_tr)\n",
    "\n",
    "predictions_aroma_2 = knn_aroma_2.predict(chi_aroma_test)\n",
    "predictions_aroma_5 = knn_aroma_5.predict(chi_aroma_test)\n",
    "predictions_aroma_7 = knn_aroma_7.predict(chi_aroma_test)\n",
    "predictions_aroma_9 = knn_aroma_9.predict(chi_aroma_test)\n",
    "predictions_aroma_13 = knn_aroma_13.predict(chi_aroma_test)\n",
    "predictions_aroma_20 = knn_aroma_20.predict(chi_aroma_test)\n",
    "predictions_aroma_50 = knn_aroma_50.predict(chi_aroma_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== PRICE ===================\n",
      "Nilai Model price K-NN pada K-2           : 51.13636363636363 %\n",
      "Nilai Model price K-NN pada K-5           : 50.63131313131313 %\n",
      "Nilai Model price K-NN pada K-7           : 50.88383838383839 %\n",
      "Nilai Model price K-NN pada K-9           : 51.26262626262626 %\n",
      "Nilai Model price K-NN pada K-13          : 50.378787878787875 %\n",
      "Nilai Model price K-NN pada K-20          : 51.13636363636363 %\n",
      "Nilai Model price K-NN pada K-50          : 52.3989898989899 %\n",
      "                                                                             \n",
      "============= PACKAGING ================\n",
      "Nilai Model packaging K-NN pada K-2       : 83.83838383838383 %\n",
      "Nilai Model packaging K-NN pada K-5       : 84.21717171717171 %\n",
      "Nilai Model packaging K-NN pada K-7       : 84.34343434343434 %\n",
      "Nilai Model packaging K-NN pada K-9       : 84.21717171717171 %\n",
      "Nilai Model packaging K-NN pada K-13      : 84.0909090909091 %\n",
      "Nilai Model packaging K-NN pada K-20      : 84.0909090909091 %\n",
      "Nilai Model packaging K-NN pada K-50      : 83.71212121212122 %\n",
      "                                                                             \n",
      "============= PRODUCT ================\n",
      "Nilai Model product K-NN pada K-2         : 39.77272727272727 %\n",
      "Nilai Model product K-NN pada K-5         : 55.05050505050505 %\n",
      "Nilai Model product K-NN pada K-7         : 61.86868686868687 %\n",
      "Nilai Model product K-NN pada K-9         : 63.888888888888886 %\n",
      "Nilai Model product K-NN pada K-13        : 61.86868686868687 %\n",
      "Nilai Model product K-NN pada K-20        : 66.66666666666666 %\n",
      "Nilai Model product K-NN pada K-50        : 67.55050505050505 %\n",
      "                                                                             \n",
      "============= AROMA ================\n",
      "Nilai Model aroma K-NN pada K-2           : 76.64141414141415 %\n",
      "Nilai Model aroma K-NN pada K-5           : 79.04040404040404 %\n",
      "Nilai Model aroma K-NN pada K-7           : 78.66161616161617 %\n",
      "Nilai Model aroma K-NN pada K-9           : 78.28282828282829 %\n",
      "Nilai Model aroma K-NN pada K-13          : 77.65151515151516 %\n",
      "Nilai Model aroma K-NN pada K-20          : 77.77777777777779 %\n",
      "Nilai Model aroma K-NN pada K-50          : 79.54545454545455 %\n"
     ]
    }
   ],
   "source": [
    "#Nilai price\n",
    "knn2_price_skor=knn_price_2.score(chi_price_test,y_price_test)\n",
    "knn5_price_skor=knn_price_5.score(chi_price_test,y_price_test)\n",
    "knn7_price_skor=knn_price_7.score(chi_price_test,y_price_test)\n",
    "knn9_price_skor=knn_price_9.score(chi_price_test,y_price_test)\n",
    "knn13_price_skor=knn_price_13.score(chi_price_test,y_price_test)\n",
    "knn20_price_skor=knn_price_20.score(chi_price_test,y_price_test)\n",
    "knn50_price_skor=knn_price_50.score(chi_price_test,y_price_test)\n",
    "\n",
    "#nilai packaging\n",
    "knn2_packaging_skor=knn_packaging_2.score(chi_packaging_test,y_packaging_test)\n",
    "knn5_packaging_skor=knn_packaging_5.score(chi_packaging_test,y_packaging_test)\n",
    "knn7_packaging_skor=knn_packaging_7.score(chi_packaging_test,y_packaging_test)\n",
    "knn9_packaging_skor=knn_packaging_9.score(chi_packaging_test,y_packaging_test)\n",
    "knn13_packaging_skor=knn_packaging_13.score(chi_packaging_test,y_packaging_test)\n",
    "knn20_packaging_skor=knn_packaging_20.score(chi_packaging_test,y_packaging_test)\n",
    "knn50_packaging_skor=knn_packaging_50.score(chi_packaging_test,y_packaging_test)\n",
    "\n",
    "#nilai product\n",
    "knn2_product_skor=knn_product_2.score(chi_product_test,y_product_test)\n",
    "knn5_product_skor=knn_product_5.score(chi_product_test,y_product_test)\n",
    "knn7_product_skor=knn_product_7.score(chi_product_test,y_product_test)\n",
    "knn9_product_skor=knn_product_9.score(chi_product_test,y_product_test)\n",
    "knn13_product_skor=knn_product_13.score(chi_product_test,y_product_test)\n",
    "knn20_product_skor=knn_product_20.score(chi_product_test,y_product_test)\n",
    "knn50_product_skor=knn_product_50.score(chi_product_test,y_product_test)\n",
    "\n",
    "#nilai aroma\n",
    "knn2_aroma_skor=knn_aroma_2.score(chi_aroma_test,y_aroma_test)\n",
    "knn5_aroma_skor=knn_aroma_5.score(chi_aroma_test,y_aroma_test)\n",
    "knn7_aroma_skor=knn_aroma_7.score(chi_aroma_test,y_aroma_test)\n",
    "knn9_aroma_skor=knn_aroma_9.score(chi_aroma_test,y_aroma_test)\n",
    "knn13_aroma_skor=knn_aroma_13.score(chi_aroma_test,y_aroma_test)\n",
    "knn20_aroma_skor=knn_aroma_20.score(chi_aroma_test,y_aroma_test)\n",
    "knn50_aroma_skor=knn_aroma_50.score(chi_aroma_test,y_aroma_test)\n",
    "\n",
    "print('=============== PRICE ===================')\n",
    "print('Nilai Model price K-NN pada K-2           :',knn2_price_skor * 100 ,'%')\n",
    "print('Nilai Model price K-NN pada K-5           :',knn5_price_skor * 100 ,'%')\n",
    "print('Nilai Model price K-NN pada K-7           :',knn7_price_skor * 100 ,'%')\n",
    "print('Nilai Model price K-NN pada K-9           :',knn9_price_skor * 100 ,'%')\n",
    "print('Nilai Model price K-NN pada K-13          :',knn13_price_skor * 100 ,'%')\n",
    "print('Nilai Model price K-NN pada K-20          :',knn20_price_skor * 100 ,'%')\n",
    "print('Nilai Model price K-NN pada K-50          :',knn50_price_skor * 100 ,'%')\n",
    "print('                                                                             ')\n",
    "print('============= PACKAGING ================')\n",
    "print('Nilai Model packaging K-NN pada K-2       :',knn2_packaging_skor * 100 ,'%')\n",
    "print('Nilai Model packaging K-NN pada K-5       :',knn5_packaging_skor * 100 ,'%')\n",
    "print('Nilai Model packaging K-NN pada K-7       :',knn7_packaging_skor * 100 ,'%')\n",
    "print('Nilai Model packaging K-NN pada K-9       :',knn9_packaging_skor * 100 ,'%')\n",
    "print('Nilai Model packaging K-NN pada K-13      :',knn13_packaging_skor * 100 ,'%')\n",
    "print('Nilai Model packaging K-NN pada K-20      :',knn20_packaging_skor * 100 ,'%')\n",
    "print('Nilai Model packaging K-NN pada K-50      :',knn50_packaging_skor * 100 ,'%')\n",
    "print('                                                                             ')\n",
    "print('============= PRODUCT ================')\n",
    "print('Nilai Model product K-NN pada K-2         :',knn2_product_skor * 100 ,'%')\n",
    "print('Nilai Model product K-NN pada K-5         :',knn5_product_skor * 100 ,'%')\n",
    "print('Nilai Model product K-NN pada K-7         :',knn7_product_skor * 100 ,'%')\n",
    "print('Nilai Model product K-NN pada K-9         :',knn9_product_skor * 100 ,'%')\n",
    "print('Nilai Model product K-NN pada K-13        :',knn13_product_skor * 100 ,'%')\n",
    "print('Nilai Model product K-NN pada K-20        :',knn20_product_skor * 100 ,'%')\n",
    "print('Nilai Model product K-NN pada K-50        :',knn50_product_skor * 100 ,'%')\n",
    "print('                                                                             ')\n",
    "print('============= AROMA ================')\n",
    "print('Nilai Model aroma K-NN pada K-2           :',knn2_aroma_skor * 100 ,'%')\n",
    "print('Nilai Model aroma K-NN pada K-5           :',knn5_aroma_skor * 100 ,'%')\n",
    "print('Nilai Model aroma K-NN pada K-7           :',knn7_aroma_skor * 100 ,'%')\n",
    "print('Nilai Model aroma K-NN pada K-9           :',knn9_aroma_skor * 100 ,'%')\n",
    "print('Nilai Model aroma K-NN pada K-13          :',knn13_aroma_skor * 100 ,'%')\n",
    "print('Nilai Model aroma K-NN pada K-20          :',knn20_aroma_skor * 100 ,'%')\n",
    "print('Nilai Model aroma K-NN pada K-50          :',knn50_aroma_skor * 100 ,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "#from sklearn.metrics import  multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 36 100  16]\n",
      " [ 61 347  11]\n",
      " [ 50 149  22]]\n",
      "\n",
      "Accuracy: 0.51\n",
      "\n",
      "Micro Precision: 0.51\n",
      "Micro Recall: 0.51\n",
      "Micro F1-score: 0.51\n",
      "\n",
      "Macro Precision: 0.43\n",
      "Macro Recall: 0.39\n",
      "Macro F1-score: 0.36\n",
      "\n",
      "Weighted Precision: 0.48\n",
      "Weighted Recall: 0.51\n",
      "Weighted F1-score: 0.45\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.24      0.24      0.24       152\n",
      "           0       0.58      0.83      0.68       419\n",
      "           1       0.45      0.10      0.16       221\n",
      "\n",
      "    accuracy                           0.51       792\n",
      "   macro avg       0.43      0.39      0.36       792\n",
      "weighted avg       0.48      0.51      0.45       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Price Pada K = 2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_price_test,predictions_price_2)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_price_test,predictions_price_2)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_2, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_2, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_price_test,predictions_price_2, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_2, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_2, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_price_test,predictions_price_2, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_2, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_2, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_price_test,predictions_price_2, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_price_test,predictions_price_2, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 23 106  23]\n",
      " [ 36 352  31]\n",
      " [ 24 171  26]]\n",
      "\n",
      "Accuracy: 0.51\n",
      "\n",
      "Micro Precision: 0.51\n",
      "Micro Recall: 0.51\n",
      "Micro F1-score: 0.51\n",
      "\n",
      "Macro Precision: 0.39\n",
      "Macro Recall: 0.37\n",
      "Macro F1-score: 0.35\n",
      "\n",
      "Weighted Precision: 0.44\n",
      "Weighted Recall: 0.51\n",
      "Weighted F1-score: 0.44\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.28      0.15      0.20       152\n",
      "           0       0.56      0.84      0.67       419\n",
      "           1       0.33      0.12      0.17       221\n",
      "\n",
      "    accuracy                           0.51       792\n",
      "   macro avg       0.39      0.37      0.35       792\n",
      "weighted avg       0.44      0.51      0.44       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Price Pada K = 5\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_price_test,predictions_price_5)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_price_test,predictions_price_5)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_5, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_5, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_price_test,predictions_price_5, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_5, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_5, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_price_test,predictions_price_5, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_5, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_5, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_price_test,predictions_price_5, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_price_test,predictions_price_5, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 23 108  21]\n",
      " [ 27 361  31]\n",
      " [ 23 179  19]]\n",
      "\n",
      "Accuracy: 0.51\n",
      "\n",
      "Micro Precision: 0.51\n",
      "Micro Recall: 0.51\n",
      "Micro F1-score: 0.51\n",
      "\n",
      "Macro Precision: 0.38\n",
      "Macro Recall: 0.37\n",
      "Macro F1-score: 0.34\n",
      "\n",
      "Weighted Precision: 0.43\n",
      "Weighted Recall: 0.51\n",
      "Weighted F1-score: 0.43\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.32      0.15      0.20       152\n",
      "           0       0.56      0.86      0.68       419\n",
      "           1       0.27      0.09      0.13       221\n",
      "\n",
      "    accuracy                           0.51       792\n",
      "   macro avg       0.38      0.37      0.34       792\n",
      "weighted avg       0.43      0.51      0.43       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Price Pada K = 7\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_price_test,predictions_price_7)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_price_test,predictions_price_7)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_7, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_7, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_price_test,predictions_price_7, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_7, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_7, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_price_test,predictions_price_7, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_7, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_7, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_price_test,predictions_price_7, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_price_test,predictions_price_7, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 23 113  16]\n",
      " [ 26 366  27]\n",
      " [ 23 181  17]]\n",
      "\n",
      "Accuracy: 0.51\n",
      "\n",
      "Micro Precision: 0.51\n",
      "Micro Recall: 0.51\n",
      "Micro F1-score: 0.51\n",
      "\n",
      "Macro Precision: 0.39\n",
      "Macro Recall: 0.37\n",
      "Macro F1-score: 0.33\n",
      "\n",
      "Weighted Precision: 0.43\n",
      "Weighted Recall: 0.51\n",
      "Weighted F1-score: 0.43\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.32      0.15      0.21       152\n",
      "           0       0.55      0.87      0.68       419\n",
      "           1       0.28      0.08      0.12       221\n",
      "\n",
      "    accuracy                           0.51       792\n",
      "   macro avg       0.39      0.37      0.33       792\n",
      "weighted avg       0.43      0.51      0.43       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Price Pada K = 9\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_price_test,predictions_price_9)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_price_test,predictions_price_9)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_9, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_9, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_price_test,predictions_price_9, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_9, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_9, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_price_test,predictions_price_9, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_9, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_9, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_price_test,predictions_price_9, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_price_test,predictions_price_9, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 19 119  14]\n",
      " [ 31 370  18]\n",
      " [ 22 189  10]]\n",
      "\n",
      "Accuracy: 0.50\n",
      "\n",
      "Micro Precision: 0.50\n",
      "Micro Recall: 0.50\n",
      "Micro F1-score: 0.50\n",
      "\n",
      "Macro Precision: 0.35\n",
      "Macro Recall: 0.35\n",
      "Macro F1-score: 0.31\n",
      "\n",
      "Weighted Precision: 0.41\n",
      "Weighted Recall: 0.50\n",
      "Weighted F1-score: 0.41\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.26      0.12      0.17       152\n",
      "           0       0.55      0.88      0.67       419\n",
      "           1       0.24      0.05      0.08       221\n",
      "\n",
      "    accuracy                           0.50       792\n",
      "   macro avg       0.35      0.35      0.31       792\n",
      "weighted avg       0.41      0.50      0.41       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Price Pada K = 13\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_price_test,predictions_price_13)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_price_test,predictions_price_13)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_13, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_13, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_price_test,predictions_price_13, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_13, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_13, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_price_test,predictions_price_13, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_13, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_13, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_price_test,predictions_price_13, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_price_test,predictions_price_13, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 14 125  13]\n",
      " [ 26 381  12]\n",
      " [ 21 190  10]]\n",
      "\n",
      "Accuracy: 0.51\n",
      "\n",
      "Micro Precision: 0.51\n",
      "Micro Recall: 0.51\n",
      "Micro F1-score: 0.51\n",
      "\n",
      "Macro Precision: 0.35\n",
      "Macro Recall: 0.35\n",
      "Macro F1-score: 0.30\n",
      "\n",
      "Weighted Precision: 0.41\n",
      "Weighted Recall: 0.51\n",
      "Weighted F1-score: 0.41\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.23      0.09      0.13       152\n",
      "           0       0.55      0.91      0.68       419\n",
      "           1       0.29      0.05      0.08       221\n",
      "\n",
      "    accuracy                           0.51       792\n",
      "   macro avg       0.35      0.35      0.30       792\n",
      "weighted avg       0.41      0.51      0.41       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Price Pada K = 20\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_price_test,predictions_price_20)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_price_test,predictions_price_20)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_20, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_20, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_price_test,predictions_price_20, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_20, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_20, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_price_test,predictions_price_20, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_20, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_20, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_price_test,predictions_price_20, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_price_test,predictions_price_20, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  9 136   7]\n",
      " [  9 400  10]\n",
      " [ 18 197   6]]\n",
      "\n",
      "Accuracy: 0.52\n",
      "\n",
      "Micro Precision: 0.52\n",
      "Micro Recall: 0.52\n",
      "Micro F1-score: 0.52\n",
      "\n",
      "Macro Precision: 0.35\n",
      "Macro Recall: 0.35\n",
      "Macro F1-score: 0.28\n",
      "\n",
      "Weighted Precision: 0.41\n",
      "Weighted Recall: 0.52\n",
      "Weighted F1-score: 0.40\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.25      0.06      0.10       152\n",
      "           0       0.55      0.95      0.69       419\n",
      "           1       0.26      0.03      0.05       221\n",
      "\n",
      "    accuracy                           0.52       792\n",
      "   macro avg       0.35      0.35      0.28       792\n",
      "weighted avg       0.41      0.52      0.40       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Price Pada K = 50\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_price_test,predictions_price_50)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_price_test,predictions_price_50)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_50, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_50, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_price_test,predictions_price_50, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_50, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_50, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_price_test,predictions_price_50, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_price_test,predictions_price_50, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_price_test,predictions_price_50, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_price_test,predictions_price_50, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_price_test,predictions_price_50, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 10  27   4]\n",
      " [ 10 649  13]\n",
      " [ 19  55   5]]\n",
      "\n",
      "Accuracy: 0.84\n",
      "\n",
      "Micro Precision: 0.84\n",
      "Micro Recall: 0.84\n",
      "Micro F1-score: 0.84\n",
      "\n",
      "Macro Precision: 0.46\n",
      "Macro Recall: 0.42\n",
      "Macro F1-score: 0.42\n",
      "\n",
      "Weighted Precision: 0.79\n",
      "Weighted Recall: 0.84\n",
      "Weighted F1-score: 0.81\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.26      0.24      0.25        41\n",
      "           0       0.89      0.97      0.93       672\n",
      "           1       0.23      0.06      0.10        79\n",
      "\n",
      "    accuracy                           0.84       792\n",
      "   macro avg       0.46      0.42      0.42       792\n",
      "weighted avg       0.79      0.84      0.81       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Packaging pada K= 2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_packaging_test,predictions_packaging_2)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_packaging_test,predictions_packaging_2)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_2, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_2, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_packaging_test,predictions_packaging_2, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_2, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_2, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_packaging_test,predictions_packaging_2, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_2, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_2, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_packaging_test,predictions_packaging_2, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_packaging_test,predictions_packaging_2, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  5  29   7]\n",
      " [  1 655  16]\n",
      " [  4  68   7]]\n",
      "\n",
      "Accuracy: 0.84\n",
      "\n",
      "Micro Precision: 0.84\n",
      "Micro Recall: 0.84\n",
      "Micro F1-score: 0.84\n",
      "\n",
      "Macro Precision: 0.53\n",
      "Macro Recall: 0.40\n",
      "Macro F1-score: 0.41\n",
      "\n",
      "Weighted Precision: 0.79\n",
      "Weighted Recall: 0.84\n",
      "Weighted F1-score: 0.80\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.50      0.12      0.20        41\n",
      "           0       0.87      0.97      0.92       672\n",
      "           1       0.23      0.09      0.13        79\n",
      "\n",
      "    accuracy                           0.84       792\n",
      "   macro avg       0.53      0.40      0.41       792\n",
      "weighted avg       0.79      0.84      0.80       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Packaging pada K= 5\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_packaging_test,predictions_packaging_5)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_packaging_test,predictions_packaging_5)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_5, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_5, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_packaging_test,predictions_packaging_5, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_5, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_5, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_packaging_test,predictions_packaging_5, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_5, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_5, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_packaging_test,predictions_packaging_5, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_packaging_test,predictions_packaging_5, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  2  32   7]\n",
      " [  0 657  15]\n",
      " [  0  70   9]]\n",
      "\n",
      "Accuracy: 0.84\n",
      "\n",
      "Micro Precision: 0.84\n",
      "Micro Recall: 0.84\n",
      "Micro F1-score: 0.84\n",
      "\n",
      "Macro Precision: 0.72\n",
      "Macro Recall: 0.38\n",
      "Macro F1-score: 0.39\n",
      "\n",
      "Weighted Precision: 0.82\n",
      "Weighted Recall: 0.84\n",
      "Weighted F1-score: 0.80\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.05      0.09        41\n",
      "           0       0.87      0.98      0.92       672\n",
      "           1       0.29      0.11      0.16        79\n",
      "\n",
      "    accuracy                           0.84       792\n",
      "   macro avg       0.72      0.38      0.39       792\n",
      "weighted avg       0.82      0.84      0.80       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Packaging pada K= 7\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_packaging_test,predictions_packaging_7)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_packaging_test,predictions_packaging_7)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_7, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_7, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_packaging_test,predictions_packaging_7, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_7, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_7, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_packaging_test,predictions_packaging_7, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_7, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_7, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_packaging_test,predictions_packaging_7, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_packaging_test,predictions_packaging_7, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  2  32   7]\n",
      " [  0 657  15]\n",
      " [  0  71   8]]\n",
      "\n",
      "Accuracy: 0.84\n",
      "\n",
      "Micro Precision: 0.84\n",
      "Micro Recall: 0.84\n",
      "Micro F1-score: 0.84\n",
      "\n",
      "Macro Precision: 0.71\n",
      "Macro Recall: 0.38\n",
      "Macro F1-score: 0.39\n",
      "\n",
      "Weighted Precision: 0.81\n",
      "Weighted Recall: 0.84\n",
      "Weighted F1-score: 0.80\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.05      0.09        41\n",
      "           0       0.86      0.98      0.92       672\n",
      "           1       0.27      0.10      0.15        79\n",
      "\n",
      "    accuracy                           0.84       792\n",
      "   macro avg       0.71      0.38      0.39       792\n",
      "weighted avg       0.81      0.84      0.80       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Packaging pada K= 9\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_packaging_test,predictions_packaging_9)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_packaging_test,predictions_packaging_9)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_9, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_9, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_packaging_test,predictions_packaging_9, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_9, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_9, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_packaging_test,predictions_packaging_9, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_9, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_9, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_packaging_test,predictions_packaging_9, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_packaging_test,predictions_packaging_9, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  1  31   9]\n",
      " [  0 657  15]\n",
      " [  0  71   8]]\n",
      "\n",
      "Accuracy: 0.84\n",
      "\n",
      "Micro Precision: 0.84\n",
      "Micro Recall: 0.84\n",
      "Micro F1-score: 0.84\n",
      "\n",
      "Macro Precision: 0.71\n",
      "Macro Recall: 0.37\n",
      "Macro F1-score: 0.37\n",
      "\n",
      "Weighted Precision: 0.81\n",
      "Weighted Recall: 0.84\n",
      "Weighted F1-score: 0.80\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.02      0.05        41\n",
      "           0       0.87      0.98      0.92       672\n",
      "           1       0.25      0.10      0.14        79\n",
      "\n",
      "    accuracy                           0.84       792\n",
      "   macro avg       0.71      0.37      0.37       792\n",
      "weighted avg       0.81      0.84      0.80       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Packaging pada K= 13\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_packaging_test,predictions_packaging_13)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_packaging_test,predictions_packaging_13)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_13, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_13, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_packaging_test,predictions_packaging_13, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_13, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_13, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_packaging_test,predictions_packaging_13, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_13, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_13, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_packaging_test,predictions_packaging_13, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_packaging_test,predictions_packaging_13, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  0  32   9]\n",
      " [  0 657  15]\n",
      " [  0  70   9]]\n",
      "\n",
      "Accuracy: 0.84\n",
      "\n",
      "Micro Precision: 0.84\n",
      "Micro Recall: 0.84\n",
      "Micro F1-score: 0.84\n",
      "\n",
      "Macro Precision: 0.38\n",
      "Macro Recall: 0.36\n",
      "Macro F1-score: 0.36\n",
      "\n",
      "Weighted Precision: 0.76\n",
      "Weighted Recall: 0.84\n",
      "Weighted F1-score: 0.80\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        41\n",
      "           0       0.87      0.98      0.92       672\n",
      "           1       0.27      0.11      0.16        79\n",
      "\n",
      "    accuracy                           0.84       792\n",
      "   macro avg       0.38      0.36      0.36       792\n",
      "weighted avg       0.76      0.84      0.80       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Packaging pada K= 20\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_packaging_test,predictions_packaging_20)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_packaging_test,predictions_packaging_20)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_20, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_20, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_packaging_test,predictions_packaging_20, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_20, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_20, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_packaging_test,predictions_packaging_20, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_20, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_20, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_packaging_test,predictions_packaging_20, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_packaging_test,predictions_packaging_20, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  0  36   5]\n",
      " [  0 659  13]\n",
      " [  0  75   4]]\n",
      "\n",
      "Accuracy: 0.84\n",
      "\n",
      "Micro Precision: 0.84\n",
      "Micro Recall: 0.84\n",
      "Micro F1-score: 0.84\n",
      "\n",
      "Macro Precision: 0.35\n",
      "Macro Recall: 0.34\n",
      "Macro F1-score: 0.33\n",
      "\n",
      "Weighted Precision: 0.74\n",
      "Weighted Recall: 0.84\n",
      "Weighted F1-score: 0.78\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        41\n",
      "           0       0.86      0.98      0.91       672\n",
      "           1       0.18      0.05      0.08        79\n",
      "\n",
      "    accuracy                           0.84       792\n",
      "   macro avg       0.35      0.34      0.33       792\n",
      "weighted avg       0.74      0.84      0.78       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Packaging pada K= 50\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_packaging_test,predictions_packaging_50)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_packaging_test,predictions_packaging_50)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_50, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_50, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_packaging_test,predictions_packaging_50, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_50, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_50, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_packaging_test,predictions_packaging_50, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_packaging_test,predictions_packaging_50, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_packaging_test,predictions_packaging_50, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_packaging_test,predictions_packaging_50, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_packaging_test,predictions_packaging_50, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 56  29  50]\n",
      " [ 41  26  54]\n",
      " [203 100 233]]\n",
      "\n",
      "Accuracy: 0.40\n",
      "\n",
      "Micro Precision: 0.40\n",
      "Micro Recall: 0.40\n",
      "Micro F1-score: 0.40\n",
      "\n",
      "Macro Precision: 0.35\n",
      "Macro Recall: 0.35\n",
      "Macro F1-score: 0.33\n",
      "\n",
      "Weighted Precision: 0.53\n",
      "Weighted Recall: 0.40\n",
      "Weighted F1-score: 0.43\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.19      0.41      0.26       135\n",
      "           0       0.17      0.21      0.19       121\n",
      "           1       0.69      0.43      0.53       536\n",
      "\n",
      "    accuracy                           0.40       792\n",
      "   macro avg       0.35      0.35      0.33       792\n",
      "weighted avg       0.53      0.40      0.43       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Product pada K= 2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_product_test,predictions_product_2)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_product_test,predictions_product_2)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_2, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_2, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_product_test,predictions_product_2, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_2, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_2, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_product_test,predictions_product_2, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_2, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_2, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_product_test,predictions_product_2, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_product_test,predictions_product_2, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 28  18  89]\n",
      " [ 24  19  78]\n",
      " [ 95  52 389]]\n",
      "\n",
      "Accuracy: 0.55\n",
      "\n",
      "Micro Precision: 0.55\n",
      "Micro Recall: 0.55\n",
      "Micro F1-score: 0.55\n",
      "\n",
      "Macro Precision: 0.37\n",
      "Macro Recall: 0.36\n",
      "Macro F1-score: 0.36\n",
      "\n",
      "Weighted Precision: 0.54\n",
      "Weighted Recall: 0.55\n",
      "Weighted F1-score: 0.54\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.19      0.21      0.20       135\n",
      "           0       0.21      0.16      0.18       121\n",
      "           1       0.70      0.73      0.71       536\n",
      "\n",
      "    accuracy                           0.55       792\n",
      "   macro avg       0.37      0.36      0.36       792\n",
      "weighted avg       0.54      0.55      0.54       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Product pada K= 5\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_product_test,predictions_product_5)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_product_test,predictions_product_5)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_5, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_5, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_product_test,predictions_product_5, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_5, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_5, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_product_test,predictions_product_5, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_5, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_5, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_product_test,predictions_product_5, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_product_test,predictions_product_5, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 29  10  96]\n",
      " [ 19  15  87]\n",
      " [ 60  30 446]]\n",
      "\n",
      "Accuracy: 0.62\n",
      "\n",
      "Micro Precision: 0.62\n",
      "Micro Recall: 0.62\n",
      "Micro F1-score: 0.62\n",
      "\n",
      "Macro Precision: 0.42\n",
      "Macro Recall: 0.39\n",
      "Macro F1-score: 0.39\n",
      "\n",
      "Weighted Precision: 0.57\n",
      "Weighted Recall: 0.62\n",
      "Weighted F1-score: 0.58\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.27      0.21      0.24       135\n",
      "           0       0.27      0.12      0.17       121\n",
      "           1       0.71      0.83      0.77       536\n",
      "\n",
      "    accuracy                           0.62       792\n",
      "   macro avg       0.42      0.39      0.39       792\n",
      "weighted avg       0.57      0.62      0.58       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Product pada K= 7\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_product_test,predictions_product_7)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_product_test,predictions_product_7)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_7, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_7, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_product_test,predictions_product_7, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_7, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_7, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_product_test,predictions_product_7, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_7, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_7, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_product_test,predictions_product_7, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_product_test,predictions_product_7, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 22   9 104]\n",
      " [ 15  10  96]\n",
      " [ 51  11 474]]\n",
      "\n",
      "Accuracy: 0.64\n",
      "\n",
      "Micro Precision: 0.64\n",
      "Micro Recall: 0.64\n",
      "Micro F1-score: 0.64\n",
      "\n",
      "Macro Precision: 0.43\n",
      "Macro Recall: 0.38\n",
      "Macro F1-score: 0.37\n",
      "\n",
      "Weighted Precision: 0.57\n",
      "Weighted Recall: 0.64\n",
      "Weighted F1-score: 0.58\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.25      0.16      0.20       135\n",
      "           0       0.33      0.08      0.13       121\n",
      "           1       0.70      0.88      0.78       536\n",
      "\n",
      "    accuracy                           0.64       792\n",
      "   macro avg       0.43      0.38      0.37       792\n",
      "weighted avg       0.57      0.64      0.58       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Product pada K= 9\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_product_test,predictions_product_9)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_product_test,predictions_product_9)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_9, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_9, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_product_test,predictions_product_9, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_9, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_9, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_product_test,predictions_product_9, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_9, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_9, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_product_test,predictions_product_9, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_product_test,predictions_product_9, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 20   7 108]\n",
      " [ 12   5 104]\n",
      " [ 62   9 465]]\n",
      "\n",
      "Accuracy: 0.62\n",
      "\n",
      "Micro Precision: 0.62\n",
      "Micro Recall: 0.62\n",
      "Micro F1-score: 0.62\n",
      "\n",
      "Macro Precision: 0.38\n",
      "Macro Recall: 0.35\n",
      "Macro F1-score: 0.34\n",
      "\n",
      "Weighted Precision: 0.54\n",
      "Weighted Recall: 0.62\n",
      "Weighted F1-score: 0.56\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.21      0.15      0.17       135\n",
      "           0       0.24      0.04      0.07       121\n",
      "           1       0.69      0.87      0.77       536\n",
      "\n",
      "    accuracy                           0.62       792\n",
      "   macro avg       0.38      0.35      0.34       792\n",
      "weighted avg       0.54      0.62      0.56       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Product pada K= 13\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_product_test,predictions_product_13)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_product_test,predictions_product_13)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_13, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_13, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_product_test,predictions_product_13, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_13, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_13, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_product_test,predictions_product_13, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_13, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_13, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_product_test,predictions_product_13, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_product_test,predictions_product_13, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 13   4 118]\n",
      " [  7   4 110]\n",
      " [ 18   7 511]]\n",
      "\n",
      "Accuracy: 0.67\n",
      "\n",
      "Micro Precision: 0.67\n",
      "Micro Recall: 0.67\n",
      "Micro F1-score: 0.67\n",
      "\n",
      "Macro Precision: 0.43\n",
      "Macro Recall: 0.36\n",
      "Macro F1-score: 0.34\n",
      "\n",
      "Weighted Precision: 0.57\n",
      "Weighted Recall: 0.67\n",
      "Weighted F1-score: 0.58\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.34      0.10      0.15       135\n",
      "           0       0.27      0.03      0.06       121\n",
      "           1       0.69      0.95      0.80       536\n",
      "\n",
      "    accuracy                           0.67       792\n",
      "   macro avg       0.43      0.36      0.34       792\n",
      "weighted avg       0.57      0.67      0.58       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Product pada K= 20\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_product_test,predictions_product_20)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_product_test,predictions_product_20)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_20, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_20, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_product_test,predictions_product_20, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_20, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_20, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_product_test,predictions_product_20, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_20, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_20, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_product_test,predictions_product_20, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_product_test,predictions_product_20, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  0   0 135]\n",
      " [  2   0 119]\n",
      " [  0   1 535]]\n",
      "\n",
      "Accuracy: 0.68\n",
      "\n",
      "Micro Precision: 0.68\n",
      "Micro Recall: 0.68\n",
      "Micro F1-score: 0.68\n",
      "\n",
      "Macro Precision: 0.23\n",
      "Macro Recall: 0.33\n",
      "Macro F1-score: 0.27\n",
      "\n",
      "Weighted Precision: 0.46\n",
      "Weighted Recall: 0.68\n",
      "Weighted F1-score: 0.55\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       135\n",
      "           0       0.00      0.00      0.00       121\n",
      "           1       0.68      1.00      0.81       536\n",
      "\n",
      "    accuracy                           0.68       792\n",
      "   macro avg       0.23      0.33      0.27       792\n",
      "weighted avg       0.46      0.68      0.55       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Product pada K= 50\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_product_test,predictions_product_50)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_product_test,predictions_product_50)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_50, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_50, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_product_test,predictions_product_50, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_50, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_50, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_product_test,predictions_product_50, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_product_test,predictions_product_50, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_product_test,predictions_product_50, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_product_test,predictions_product_50, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_product_test,predictions_product_50, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 12  25   3]\n",
      " [ 14 590  11]\n",
      " [ 22 110   5]]\n",
      "\n",
      "Accuracy: 0.77\n",
      "\n",
      "Micro Precision: 0.77\n",
      "Micro Recall: 0.77\n",
      "Micro F1-score: 0.77\n",
      "\n",
      "Macro Precision: 0.44\n",
      "Macro Recall: 0.43\n",
      "Macro F1-score: 0.41\n",
      "\n",
      "Weighted Precision: 0.69\n",
      "Weighted Recall: 0.77\n",
      "Weighted F1-score: 0.71\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.25      0.30      0.27        40\n",
      "           0       0.81      0.96      0.88       615\n",
      "           1       0.26      0.04      0.06       137\n",
      "\n",
      "    accuracy                           0.77       792\n",
      "   macro avg       0.44      0.43      0.41       792\n",
      "weighted avg       0.69      0.77      0.71       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Aroma Pada K=2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_aroma_test,predictions_aroma_2)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_aroma_test,predictions_aroma_2)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_2, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_2, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_aroma_test,predictions_aroma_2, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_2, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_2, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_aroma_test,predictions_aroma_2, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_2, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_2, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_aroma_test,predictions_aroma_2, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_aroma_test,predictions_aroma_2, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  8  24   8]\n",
      " [  0 614   1]\n",
      " [  1 132   4]]\n",
      "\n",
      "Accuracy: 0.79\n",
      "\n",
      "Micro Precision: 0.79\n",
      "Micro Recall: 0.79\n",
      "Micro F1-score: 0.79\n",
      "\n",
      "Macro Precision: 0.66\n",
      "Macro Recall: 0.41\n",
      "Macro F1-score: 0.42\n",
      "\n",
      "Weighted Precision: 0.72\n",
      "Weighted Recall: 0.79\n",
      "Weighted F1-score: 0.71\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.89      0.20      0.33        40\n",
      "           0       0.80      1.00      0.89       615\n",
      "           1       0.31      0.03      0.05       137\n",
      "\n",
      "    accuracy                           0.79       792\n",
      "   macro avg       0.66      0.41      0.42       792\n",
      "weighted avg       0.72      0.79      0.71       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Aroma Pada K=5\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_aroma_test,predictions_aroma_5)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_aroma_test,predictions_aroma_5)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_5, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_5, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_aroma_test,predictions_aroma_5, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_5, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_5, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_aroma_test,predictions_aroma_5, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_5, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_5, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_aroma_test,predictions_aroma_5, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_aroma_test,predictions_aroma_5, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  6  27   7]\n",
      " [  0 614   1]\n",
      " [  2 132   3]]\n",
      "\n",
      "Accuracy: 0.79\n",
      "\n",
      "Micro Precision: 0.79\n",
      "Micro Recall: 0.79\n",
      "Micro F1-score: 0.79\n",
      "\n",
      "Macro Precision: 0.61\n",
      "Macro Recall: 0.39\n",
      "Macro F1-score: 0.39\n",
      "\n",
      "Weighted Precision: 0.70\n",
      "Weighted Recall: 0.79\n",
      "Weighted F1-score: 0.71\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.15      0.25        40\n",
      "           0       0.79      1.00      0.88       615\n",
      "           1       0.27      0.02      0.04       137\n",
      "\n",
      "    accuracy                           0.79       792\n",
      "   macro avg       0.61      0.39      0.39       792\n",
      "weighted avg       0.70      0.79      0.71       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Aroma Pada K=7\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_aroma_test,predictions_aroma_7)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_aroma_test,predictions_aroma_7)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_7, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_7, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_aroma_test,predictions_aroma_7, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_7, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_7, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_aroma_test,predictions_aroma_7, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_7, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_7, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_aroma_test,predictions_aroma_7, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_aroma_test,predictions_aroma_7, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  5  27   8]\n",
      " [  0 614   1]\n",
      " [  3 133   1]]\n",
      "\n",
      "Accuracy: 0.78\n",
      "\n",
      "Micro Precision: 0.78\n",
      "Micro Recall: 0.78\n",
      "Micro F1-score: 0.78\n",
      "\n",
      "Macro Precision: 0.51\n",
      "Macro Recall: 0.38\n",
      "Macro F1-score: 0.37\n",
      "\n",
      "Weighted Precision: 0.66\n",
      "Weighted Recall: 0.78\n",
      "Weighted F1-score: 0.70\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.62      0.12      0.21        40\n",
      "           0       0.79      1.00      0.88       615\n",
      "           1       0.10      0.01      0.01       137\n",
      "\n",
      "    accuracy                           0.78       792\n",
      "   macro avg       0.51      0.38      0.37       792\n",
      "weighted avg       0.66      0.78      0.70       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Aroma Pada K=9\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_aroma_test,predictions_aroma_9)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_aroma_test,predictions_aroma_9)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_9, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_9, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_aroma_test,predictions_aroma_9, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_9, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_9, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_aroma_test,predictions_aroma_9, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_9, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_9, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_aroma_test,predictions_aroma_9, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_aroma_test,predictions_aroma_9, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  2  33   5]\n",
      " [  0 613   2]\n",
      " [  1 136   0]]\n",
      "\n",
      "Accuracy: 0.78\n",
      "\n",
      "Micro Precision: 0.78\n",
      "Micro Recall: 0.78\n",
      "Micro F1-score: 0.78\n",
      "\n",
      "Macro Precision: 0.48\n",
      "Macro Recall: 0.35\n",
      "Macro F1-score: 0.32\n",
      "\n",
      "Weighted Precision: 0.64\n",
      "Weighted Recall: 0.78\n",
      "Weighted F1-score: 0.69\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.05      0.09        40\n",
      "           0       0.78      1.00      0.88       615\n",
      "           1       0.00      0.00      0.00       137\n",
      "\n",
      "    accuracy                           0.78       792\n",
      "   macro avg       0.48      0.35      0.32       792\n",
      "weighted avg       0.64      0.78      0.69       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Aroma Pada K=13\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_aroma_test,predictions_aroma_13)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_aroma_test,predictions_aroma_13)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_13, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_13, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_aroma_test,predictions_aroma_13, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_13, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_13, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_aroma_test,predictions_aroma_13, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_13, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_13, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_aroma_test,predictions_aroma_13, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_aroma_test,predictions_aroma_13, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  2  34   4]\n",
      " [  0 613   2]\n",
      " [  1 135   1]]\n",
      "\n",
      "Accuracy: 0.78\n",
      "\n",
      "Micro Precision: 0.78\n",
      "Micro Recall: 0.78\n",
      "Micro F1-score: 0.78\n",
      "\n",
      "Macro Precision: 0.53\n",
      "Macro Recall: 0.35\n",
      "Macro F1-score: 0.33\n",
      "\n",
      "Weighted Precision: 0.67\n",
      "Weighted Recall: 0.78\n",
      "Weighted F1-score: 0.69\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.05      0.09        40\n",
      "           0       0.78      1.00      0.88       615\n",
      "           1       0.14      0.01      0.01       137\n",
      "\n",
      "    accuracy                           0.78       792\n",
      "   macro avg       0.53      0.35      0.33       792\n",
      "weighted avg       0.67      0.78      0.69       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Aroma Pada K=20\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_aroma_test,predictions_aroma_20)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_aroma_test,predictions_aroma_20)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_20, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_20, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_aroma_test,predictions_aroma_20, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_20, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_20, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_aroma_test,predictions_aroma_20, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_20, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_20, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_aroma_test,predictions_aroma_20, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_aroma_test,predictions_aroma_20, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  2  34   4]\n",
      " [  0 614   1]\n",
      " [  0 123  14]]\n",
      "\n",
      "Accuracy: 0.80\n",
      "\n",
      "Micro Precision: 0.80\n",
      "Micro Recall: 0.80\n",
      "Micro F1-score: 0.80\n",
      "\n",
      "Macro Precision: 0.84\n",
      "Macro Recall: 0.38\n",
      "Macro F1-score: 0.39\n",
      "\n",
      "Weighted Precision: 0.80\n",
      "Weighted Recall: 0.80\n",
      "Weighted F1-score: 0.72\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.05      0.10        40\n",
      "           0       0.80      1.00      0.89       615\n",
      "           1       0.74      0.10      0.18       137\n",
      "\n",
      "    accuracy                           0.80       792\n",
      "   macro avg       0.84      0.38      0.39       792\n",
      "weighted avg       0.80      0.80      0.72       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi aspek Aroma Pada K=50\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_aroma_test,predictions_aroma_50)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_aroma_test,predictions_aroma_50)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_50, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_50, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_aroma_test,predictions_aroma_50, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_50, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_50, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_aroma_test,predictions_aroma_50, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_aroma_test,predictions_aroma_50, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_aroma_test,predictions_aroma_50, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_aroma_test,predictions_aroma_50, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_aroma_test,predictions_aroma_50, target_names=['-1', '0', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
